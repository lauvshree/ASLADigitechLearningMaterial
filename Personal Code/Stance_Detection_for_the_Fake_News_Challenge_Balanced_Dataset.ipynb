{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Stance_Detection_for_the_Fake_News_Challenge_Balanced_Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QI9jhXKPCFcJ"
      },
      "source": [
        "# Stance Detection for the Fake News Challenge\n",
        "\n",
        "## Identifying Textual Relationships with Deep Neural Nets\n",
        "\n",
        "### Check the problem context [here](https://drive.google.com/open?id=1KfWaZyQdGBw8AUTacJ2yY86Yxgw2Xwq0).\n",
        "\n",
        "### Download files required for the project from [here](https://drive.google.com/open?id=10yf39ifEwVihw4xeJJR60oeFBY30Y5J8)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vSNgdEMpenpE"
      },
      "source": [
        "## Step1: Load the given dataset  \n",
        "\n",
        "1. Mount the google drive\n",
        "\n",
        "2. Import Glove embeddings\n",
        "\n",
        "3. Import the test and train datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aPOZRohMiSpQ"
      },
      "source": [
        "### Mount the google drive to access required project files\n",
        "\n",
        "Run the below commands"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7AS39z1XgFpT",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S_7yCFdzgFsH",
        "outputId": "6b82ee58-3a26-4b9f-cf49-6ac85c8d3718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bhZdJ4zpwWzN"
      },
      "source": [
        "#### Path for Project files on google drive\n",
        "\n",
        "**Note:** You need to change this path according where you have kept the files in google drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Aol97RUogFuS",
        "colab": {}
      },
      "source": [
        "project_path = \"/content/drive/My Drive/Project 12/Dataset/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TjLJEQ_PwcGi"
      },
      "source": [
        "We will reuse the same dataset that we loaded in the imbalanced dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7gXO1WZ-gFwm",
        "outputId": "cf64b80b-7c76-4deb-e131-e19cadb10ccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "%cd /content/drive/My Drive/Project\\ 12/Dataset/\n",
        "%ls\n",
        "bodies = pd.read_csv(\"./train_bodies.csv\")\n",
        "stances = pd.read_csv(\"./train_stances.csv\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Project 12/Dataset\n",
            "glove.6B.zip  train_bodies.csv     train_stances.csv\n",
            "\u001b[0m\u001b[01;34mglovefiles\u001b[0m/   train_bodies.gsheet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kosAWskdOOT8",
        "outputId": "08e70b0e-47a2-4bd1-e6c7-61d6e274fae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "bodies.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Last week we hinted at what was to come as Ebo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>Posting photos of a gun-toting child online, I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>At least 25 suspected Boko Haram insurgents we...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID                                        articleBody\n",
              "0        0  A small meteorite crashed into a wooded area i...\n",
              "1        4  Last week we hinted at what was to come as Ebo...\n",
              "2        5  (NEWSER) – Wonder how long a Quarter Pounder w...\n",
              "3        6  Posting photos of a gun-toting child online, I...\n",
              "4        7  At least 25 suspected Boko Haram insurgents we..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g4ycQbBCg20S"
      },
      "source": [
        "\n",
        "<h2> Check1:</h2>\n",
        "  \n",
        "<h3> You should see the below output if you run `dataset.head()` command as given below </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IUtF7iOmj11k",
        "outputId": "357024c7-b73f-4c50-b403-3c338d099776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "dataset = pd.merge(bodies, stances, on='Body ID')\n",
        "dataset.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID  ...     Stance\n",
              "0        0  ...  unrelated\n",
              "1        0  ...  unrelated\n",
              "2        0  ...  unrelated\n",
              "3        0  ...  unrelated\n",
              "4        0  ...  unrelated\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5x1gm7ny8Ex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "0531edf7-9578-442a-d6e6-6b6321272f29"
      },
      "source": [
        "dataset.Stance.value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "unrelated    36545\n",
              "discuss       8909\n",
              "agree         3678\n",
              "disagree       840\n",
              "Name: Stance, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaNnHhHuy1jd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "98644f53-98b7-4ead-b68c-f3c93ca32b2b"
      },
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "df_unrelated = dataset[dataset.Stance == 'unrelated']\n",
        "df_discuss = dataset[dataset.Stance == 'discuss']\n",
        "df_agree = dataset[dataset.Stance == 'agree']\n",
        "df_disagree = dataset[dataset.Stance == 'disagree']\n",
        " \n",
        "df_unrelated = resample(df_unrelated, \n",
        "                                 replace=True, \n",
        "                                 n_samples=10000,\n",
        "                                 random_state=5)\n",
        " \n",
        "df_discuss = resample(df_discuss, \n",
        "                                 replace=True, \n",
        "                                 n_samples=10000,\n",
        "                                 random_state=5)\n",
        "\n",
        "df_agree = resample(df_agree, \n",
        "                                 replace=True, \n",
        "                                 n_samples=10000,\n",
        "                                 random_state=5)\n",
        "\n",
        "df_disagree = resample(df_disagree, \n",
        "                                 replace=True, \n",
        "                                 n_samples=10000,\n",
        "                                 random_state=5)\n",
        "\n",
        "dataset = pd.concat([df_unrelated, df_discuss, df_agree, df_disagree])\n",
        "# Display new class counts\n",
        "dataset.Stance.value_counts()\n",
        "\n",
        "dataset.head(10)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49054</th>\n",
              "      <td>2498</td>\n",
              "      <td>In case you missed it, Vogue Magazine, one of ...</td>\n",
              "      <td>Pumpkin Spice Condoms Could Be The Only Thing ...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40454</th>\n",
              "      <td>2127</td>\n",
              "      <td>KANSAS CITY, Mo. - Kansas City health official...</td>\n",
              "      <td>ISIS Reportedly Beheads American Photojournali...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13322</th>\n",
              "      <td>800</td>\n",
              "      <td>Well, here’s the creepiest thing you’ll read a...</td>\n",
              "      <td>Nigeria Boko Haram blamed for raids despite tr...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18525</th>\n",
              "      <td>1094</td>\n",
              "      <td>On Saturday, the entire Internet watched in ho...</td>\n",
              "      <td>Say 'eh-oh!' to the Teletubbies SUN BABY - can...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10340</th>\n",
              "      <td>633</td>\n",
              "      <td>Knightscope co-founder Stacy Stephens said rum...</td>\n",
              "      <td>Media outlets identify 'Jihadi John'</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4690</th>\n",
              "      <td>269</td>\n",
              "      <td>Islamic State militants have released a video ...</td>\n",
              "      <td>Did Kim Yo-Jong Take Kim Jong Un’s Role? North...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8564</th>\n",
              "      <td>527</td>\n",
              "      <td>A hallucinogenic fungi has been found growing ...</td>\n",
              "      <td>IRAQI AND KURDISH MEDIA REPORTS: ISIS FIGHTERS...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37591</th>\n",
              "      <td>2009</td>\n",
              "      <td>Twitter is abuzz with rumours that Cuba's form...</td>\n",
              "      <td>U.S. accidentally delivered weapons to the Isl...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33151</th>\n",
              "      <td>1826</td>\n",
              "      <td>One passenger at Dulles International Airport ...</td>\n",
              "      <td>Tiger Woods prices private island at $7.1 million</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47307</th>\n",
              "      <td>2412</td>\n",
              "      <td>Reporting in the Telegraph states that US dron...</td>\n",
              "      <td>That powerful Lego letter to parents from the ...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Body ID  ...     Stance\n",
              "49054     2498  ...  unrelated\n",
              "40454     2127  ...  unrelated\n",
              "13322      800  ...  unrelated\n",
              "18525     1094  ...  unrelated\n",
              "10340      633  ...  unrelated\n",
              "4690       269  ...  unrelated\n",
              "8564       527  ...  unrelated\n",
              "37591     2009  ...  unrelated\n",
              "33151     1826  ...  unrelated\n",
              "47307     2412  ...  unrelated\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tjzVz2ifijmj"
      },
      "source": [
        "## Step2: Data Pre-processing and setting some hyper parameters needed for model\n",
        "\n",
        "\n",
        "#### Run the code given below to set the required parameters.\n",
        "\n",
        "1. `MAX_SENTS` = Maximum no.of sentences to consider in an article.\n",
        "\n",
        "2. `MAX_SENT_LENGTH` = Maximum no.of words to consider in a sentence.\n",
        "\n",
        "3. `MAX_NB_WORDS` = Maximum no.of words in the total vocabualry.\n",
        "\n",
        "4. `MAX_SENTS_HEADING` = Maximum no.of sentences to consider in a heading of an article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KDXSdpvqjuqw",
        "colab": {}
      },
      "source": [
        "MAX_NB_WORDS = 20000\n",
        "MAX_SENTS = 20\n",
        "MAX_SENTS_HEADING = 1\n",
        "MAX_SENT_LENGTH = 20\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zwE7CPHdiDT-"
      },
      "source": [
        "### Download the `Punkt` from nltk using the commands given below. This is for sentence tokenization.\n",
        "\n",
        "For more info on how to use it, read [this](https://stackoverflow.com/questions/35275001/use-of-punktsentencetokenizer-in-nltk).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lsiKmyJUZ-hU",
        "outputId": "297ab2ff-959b-49bb-89d4-65d710768e5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gqwm_GbwwnhX"
      },
      "source": [
        "# Tokenizing the text and loading the pre-trained Glove word embeddings for each token  [5 marks] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WfZLR24mm32k"
      },
      "source": [
        "Keras provides [Tokenizer API](https://keras.io/preprocessing/text/) for preparing text. Read it before going any further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fLSn9S-5oG4Z"
      },
      "source": [
        "#### Import the Tokenizer from keras preprocessing text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S-VUgh2yoMlR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "daf9a96a-b200-49be-aa65-1ab23edab1bd"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eml0Lge4oOuh"
      },
      "source": [
        "#### Initialize the Tokenizer class with maximum vocabulary count as `MAX_NB_WORDS` initialized at the start of step2. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qm85qirPofc2",
        "colab": {}
      },
      "source": [
        "t = Tokenizer(num_words=MAX_NB_WORDS,filters= '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}\\n“~')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HBe1KuXDosJ7"
      },
      "source": [
        "#### Now, using fit_on_texts() from Tokenizer class, lets encode the data \n",
        "\n",
        "Note: We need to fit articleBody and Headline also to cover all the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q5rk-UyBlmyA",
        "colab": {}
      },
      "source": [
        "t.fit_on_texts(dataset['articleBody'])\n",
        "t.fit_on_texts(dataset['Headline'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "omptHX-JpBsN"
      },
      "source": [
        "#### fit_on_texts() gives the following attributes in the output as given [here](https://faroit.github.io/keras-docs/1.2.2/preprocessing/text/).\n",
        "\n",
        "* **word_counts:** dictionary mapping words (str) to the number of times they appeared on during fit. Only set after fit_on_texts was called.\n",
        "\n",
        "* **word_docs:** dictionary mapping words (str) to the number of documents/texts they appeared on during fit. Only set after fit_on_texts was called.\n",
        "\n",
        "* **word_index:** dictionary mapping words (str) to their rank/index (int). Only set after fit_on_texts was called.\n",
        "\n",
        "* **document_count:** int. Number of documents (texts/sequences) the tokenizer was trained on. Only set after fit_on_texts or fit_on_sequences was called.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SHnsT2sTtFAA"
      },
      "source": [
        "### Now, tokenize the sentences using nltk sent_tokenize() and encode the senteces with the ids we got form the above `t.word_index`\n",
        "\n",
        "Initialise 2 lists with names `texts` and `articles`.\n",
        "\n",
        "```\n",
        "texts = [] to store text of article as it is.\n",
        "\n",
        "articles = [] split the above text into a list of sentences.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ctEu-d4c4EZs",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "\n",
        "import numpy as np\n",
        "texts = np.array(dataset.articleBody)\n",
        "\n",
        "articles = []\n",
        "\n",
        "for wholeArticle in texts:\n",
        "  articles.append(sent_tokenize(wholeArticle))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "koTVJjoO6P78"
      },
      "source": [
        "## Check 2:\n",
        "\n",
        "first element of texts and articles should be as given below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b62_Ng4URosZ",
        "colab_type": "code",
        "outputId": "a4c10471-1820-40d7-c138-4b45cb7f600e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "texts[0]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In case you missed it, Vogue Magazine, one of the most glamorous institutions in the country has been dealing with the least glamorous issue ever: a rat infestation.\\n\\nThe rodents have literally been living it up in Vogue’s new luxurious digs at 1 World Trade Center in New York City. Reportedly, the rats took up residence in Anna Wintour’s office and have moved into the magazine’s world famous accessories closet.\\n\\nGawker reported that the critters have made Vogue’s infamous editor-in-chief scared to enter her office without taking precautions first.\\n\\n“The infestation is so acute, one source said, that the fashion title’s editor-in-chief, Anna Wintour, recently issued a standing order: Staffers must ensure that her personal office is rat-free before she enters it…”\\n\\nA source told People that: “the girls that work there see the droppings everywhere. It’s nasty.”\\n\\nThe rats have also reportedly eaten holes into shoe boxes and left droppings on the floor of the accessories closet. It’s believed that Vogue’s newest office mates made their way into the office through the ventilation system.\\n\\nAnna Wintours New Office in One WTC via Hamish Bowles Instagram\\n\\n (Photo via Instagram)\\n\\nAnna Wintour’s new office. \\n\\nCall me petty, but after this headline made me gag, it also made me laugh hysterically. I know that vermin infestations are no laughing matter but there are just too many hilarious visuals that this headline conjures up.\\n\\nI just keep picturing rats happily nibbling on Michael Kors dresses as if they’re camembert cheese. I also can’t get the image of Anna Wintour frantically directing Vogue staffers who are on their hands and knees to verify that her office is rat-free. It seems like something from an episode of Ugly Betty or a scene from the sequel to The Devil Wears Prada (which I would totally watch by the way).\\n\\nVogue Staffers Packing Up Via Vogue Instagram\\n\\n(Photo via Instagram)\\n\\nIf you’re worried about how Anna will deal with this crisis, never fear. Apparently Ms. Wintour is calling in the Olivia Pope of rat exterminators.\\n\\nA source allegedly told Radar.com the following:\\n\\n“Anna isn’t amused and top pest control experts have been hired to get the problem dealt with quickly.”\\n\\nI guess the situation is getting handled. In the meantime, I’ll just keep picturing rats chasing models around the halls of Vogue’s new offices.\\n\\nYou can reach this post's author, J. Assita Camara, on twitter.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xco0B8SGRsXm",
        "colab_type": "code",
        "outputId": "965ba9cf-7d2a-4736-aa1b-ad91e87602d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "articles[0]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In case you missed it, Vogue Magazine, one of the most glamorous institutions in the country has been dealing with the least glamorous issue ever: a rat infestation.',\n",
              " 'The rodents have literally been living it up in Vogue’s new luxurious digs at 1 World Trade Center in New York City.',\n",
              " 'Reportedly, the rats took up residence in Anna Wintour’s office and have moved into the magazine’s world famous accessories closet.',\n",
              " 'Gawker reported that the critters have made Vogue’s infamous editor-in-chief scared to enter her office without taking precautions first.',\n",
              " '“The infestation is so acute, one source said, that the fashion title’s editor-in-chief, Anna Wintour, recently issued a standing order: Staffers must ensure that her personal office is rat-free before she enters it…”\\n\\nA source told People that: “the girls that work there see the droppings everywhere.',\n",
              " 'It’s nasty.”\\n\\nThe rats have also reportedly eaten holes into shoe boxes and left droppings on the floor of the accessories closet.',\n",
              " 'It’s believed that Vogue’s newest office mates made their way into the office through the ventilation system.',\n",
              " 'Anna Wintours New Office in One WTC via Hamish Bowles Instagram\\n\\n (Photo via Instagram)\\n\\nAnna Wintour’s new office.',\n",
              " 'Call me petty, but after this headline made me gag, it also made me laugh hysterically.',\n",
              " 'I know that vermin infestations are no laughing matter but there are just too many hilarious visuals that this headline conjures up.',\n",
              " 'I just keep picturing rats happily nibbling on Michael Kors dresses as if they’re camembert cheese.',\n",
              " 'I also can’t get the image of Anna Wintour frantically directing Vogue staffers who are on their hands and knees to verify that her office is rat-free.',\n",
              " 'It seems like something from an episode of Ugly Betty or a scene from the sequel to The Devil Wears Prada (which I would totally watch by the way).',\n",
              " 'Vogue Staffers Packing Up Via Vogue Instagram\\n\\n(Photo via Instagram)\\n\\nIf you’re worried about how Anna will deal with this crisis, never fear.',\n",
              " 'Apparently Ms. Wintour is calling in the Olivia Pope of rat exterminators.',\n",
              " 'A source allegedly told Radar.com the following:\\n\\n“Anna isn’t amused and top pest control experts have been hired to get the problem dealt with quickly.”\\n\\nI guess the situation is getting handled.',\n",
              " 'In the meantime, I’ll just keep picturing rats chasing models around the halls of Vogue’s new offices.',\n",
              " \"You can reach this post's author, J. Assita Camara, on twitter.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fpuRIA7cCfcY"
      },
      "source": [
        "# Now iterate through each article and each sentence to encode the words into ids using t.word_index  [5 marks] \n",
        "\n",
        "Here, to get words from sentence you can use `text_to_word_sequence` from keras preprocessing text.\n",
        "\n",
        "1. Import text_to_word_sequence\n",
        "\n",
        "2. Initialize a variable of shape (no.of articles, MAX_SENTS, MAX_SENT_LENGTH) with name `data` with zeros first (you can use numpy [np.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html) to initialize with all zeros)and then update it while iterating through the words and sentences in each article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YVyClBULCqWj",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "data = np.zeros((len(articles),MAX_SENTS,MAX_SENT_LENGTH),dtype=int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoM9YnxsVVMh",
        "colab_type": "code",
        "outputId": "169f3061-7047-47f3-87a2-cc49c8ba98ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 20, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJWRlfgSVd7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i=0\n",
        "for article in articles:\n",
        "  j=0\n",
        "  for sentence in article:\n",
        "    if j < 20:\n",
        "      wordArr =  text_to_word_sequence(sentence,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“', lower=True, split=' ')\n",
        "      k=0\n",
        "      for word in wordArr:\n",
        "        if k < 20:\n",
        "          data[i,j,k] = round(t.word_index[word])\n",
        "          k+=1\n",
        "      j+=1\n",
        "  i+=1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bFdmiDYcE144"
      },
      "source": [
        "### Check 3:\n",
        "\n",
        "Accessing first element in data should give something like given below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TsFWW5C2Djog",
        "outputId": "004b6735-ba67-45a1-8842-2eb347863468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data[0, :, :]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    5,   326,    50,  2016,    13,  1085,   884,    41,     4,\n",
              "            1,   196,  7055,  3702,     5,     1,   305,    21,    29,\n",
              "         6008,    15],\n",
              "       [    1,  3240,    17,  2165,    29,   680,    13,    42,     5,\n",
              "         4767,    62,  7317,  5868,    22,   339,   144,  1882,   700,\n",
              "            5,    62],\n",
              "       [  226,     1,  1235,   248,    42,  3962,     5,  1180,  7145,\n",
              "          390,     6,    17,  1108,    80,     1,  9443,   144,  1377,\n",
              "         5479,  5321],\n",
              "       [ 3407,    95,     7,     1, 13222,    17,   112,  4767,  3352,\n",
              "         1147,     5,   489,  2509,     3,  2269,    67,   390,   556,\n",
              "          572,  6554],\n",
              "       [    1,  2377,     9,    69,  4509,    41,   251,    14,     7,\n",
              "            1,  2327, 11783,  1147,     5,   489,  1180,  1298,   506,\n",
              "          934,     2],\n",
              "       [  156,  5484,    28,     1,  1235,    17,    53,   226,  3353,\n",
              "         5201,    80,  8512,  1943,     6,   208,  3498,    10,     1,\n",
              "         2350,     4],\n",
              "       [  156,   238,     7,  4767,  7004,   390,  8360,   112,    51,\n",
              "          232,    80,     1,   390,   124,     1, 10466,  1278,     0,\n",
              "            0,     0],\n",
              "       [ 1180, 13224,    62,   390,     5,    41,  5567,   472,  6552,\n",
              "        13225,  1093,   218,   472,  1093,  1180,  7145,    62,   390,\n",
              "            0,     0],\n",
              "       [  531,   203, 10549,    30,    36,    26,  1426,   112,   203,\n",
              "        11747,    13,    53,   112,   203,  5215, 13226,     0,     0,\n",
              "            0,     0],\n",
              "       [   35,   175,     7,  6218, 13227,    27,    46,  2721,  1202,\n",
              "           30,    49,    27,    78,   519,   164,  3750, 12945,     7,\n",
              "           26,  1426],\n",
              "       [   35,    78,   792,  5203,  1235,  9636, 12788,    10,   215,\n",
              "        13229, 13230,    19,    57,  1240, 13231,  1616,     0,     0,\n",
              "            0,     0],\n",
              "       [   35,    53,  1287,   141,     1,   468,     4,  1180,  1298,\n",
              "         9707,  5660,  1085,  3170,    31,    27,    10,    51,   537,\n",
              "            6,  3197],\n",
              "       [   13,   613,    81,   189,    20,    23,  1968,     4,  7536,\n",
              "        13232,    40,     2,   828,    20,     1,  5234,     3,     1,\n",
              "        12238,  4068],\n",
              "       [ 1085,  3170, 13233,    42,   472,  1085,  1093,   218,   472,\n",
              "         1093,    57,  1914,  2993,    37,   140,  1180,    44,   602,\n",
              "           15,    26],\n",
              "       [  418,  1944,  1298,     9,  1403,     5,     1, 13234,   438,\n",
              "            4,  1581, 13235,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    2,   251,   511,    52,  3277,   260,     1,   380,  1180,\n",
              "         1153,  7364,     6,   712,  7943,   354,   540,    17,    29,\n",
              "         3659,     3],\n",
              "       [    5,     1,  5357,  3515,    78,   792,  5203,  1235,  8012,\n",
              "         1894,   166,     1,  6660,     4,  4767,    62,  1156,     0,\n",
              "            0,     0],\n",
              "       [   50,    97,  1490,    26,  7954,  2652,  1331, 13236, 13237,\n",
              "           10,   165,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hTG6JySHehkT"
      },
      "source": [
        "# Repeat the same process for the `Headings` as well. Use variables with names `texts_heading` and `articles_heading` accordingly. [5 marks] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_CliiIhLemJV",
        "colab": {}
      },
      "source": [
        "texts_headings = dataset[\"Headline\"]\n",
        "\n",
        "article_headings = []\n",
        "\n",
        "for text_heading in texts_headings:\n",
        "  article_headings.append(sent_tokenize(text_heading))\n",
        "\n",
        "data_headline = np.zeros((len(article_headings),MAX_SENTS_HEADING,MAX_SENT_LENGTH),dtype=int)\n",
        "\n",
        "i=0\n",
        "for article_heading in article_headings:\n",
        "  j=0\n",
        "  for sentence in article_heading:\n",
        "    if j < 1:\n",
        "      wordArr =  text_to_word_sequence(sentence,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“', lower=True, split=' ')\n",
        "      k=0\n",
        "      for word in wordArr:\n",
        "        if k < 20:\n",
        "          data_headline[i,j,k] = round(t.word_index[word])\n",
        "          k+=1\n",
        "      j+=1\n",
        "  i+=1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8hKU1vP1yz1",
        "colab_type": "code",
        "outputId": "548930ed-010f-42b5-b1dc-92724697bc60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data_headline[0,:,:]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 904,  812, 2104,   76,   24,    1,  125,  453,    3, 1788,    1,\n",
              "         144,   20, 3846,  505,    0,    0,    0,    0,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iaH0Ey1qe_Co"
      },
      "source": [
        "### Now the features are ready, lets make the labels ready for the model to process.\n",
        "\n",
        "### Convert labels into one-hot vectors\n",
        "\n",
        "You can use [get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) in pandas to create one-hot vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "40mA8FI2fcxZ"
      },
      "source": [
        "### Check 4:\n",
        "\n",
        "The shape of data and labels should match the revised data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sDOxHdR3frDu"
      },
      "source": [
        "### Shuffle the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Ra-yYTvfzRt",
        "colab": {}
      },
      "source": [
        "indices = np.arange(data.shape[0])\n",
        "## shuffle the numbers\n",
        "np.random.shuffle(indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LKnSqwIFf3Iy",
        "colab": {}
      },
      "source": [
        "## shuffle the data\n",
        "data = data[indices]\n",
        "data_headline = data_headline[indices]\n",
        "## shuffle the labels according to data\n",
        "\n",
        "targets = pd.Series(stances)\n",
        "one_hot = pd.get_dummies(targets, sparse = True)\n",
        "one_hot_labels = np.asarray(one_hot)\n",
        "labels = one_hot_labels\n",
        "\n",
        "labels = labels[indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC8Vr__r7X1X",
        "colab_type": "code",
        "outputId": "18cf5fe3-de09-48df-f63d-2526fde751c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (40000, 20, 20)\n",
            "Shape of label tensor: (40000, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JcOFVfPBf9kA"
      },
      "source": [
        "### Split into train and validation sets. Split the train set 80:20 ratio to get the train and validation sets.\n",
        "\n",
        "\n",
        "Use the variable names as given below:\n",
        "\n",
        "x_train, x_val - for body of articles.\n",
        "\n",
        "x-heading_train, x_heading_val - for heading of articles.\n",
        "\n",
        "y_train - for training labels.\n",
        "\n",
        "y_val - for validation labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o5u3PTz3gEV-",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, X_heading_train, X_heading_val, y_train,y_val = train_test_split(data,data_headline, labels, test_size = 0.20, random_state=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UTyvoHrsgMDw"
      },
      "source": [
        "### Check 5:\n",
        "\n",
        "The shape of x_train, x_val, y_train and y_val should match the revised dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KLEbiw2Yghe2",
        "outputId": "8c860f39-7729-455b-e418-cb9d40cd5f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_heading_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(X_val.shape)\n",
        "print(X_heading_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32000, 20, 20)\n",
            "(32000, 1, 20)\n",
            "(32000, 4)\n",
            "(8000, 20, 20)\n",
            "(8000, 1, 20)\n",
            "(8000, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yNnoBtArhJ1E"
      },
      "source": [
        "### Create embedding matrix with the glove embeddings\n",
        "\n",
        "\n",
        "Run the below code to create embedding_matrix which has all the words and their glove embedding if present in glove word list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eKqn2IL2ZF8v",
        "outputId": "a9df6571-3bc0-48e9-adb1-11230875d39a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open(project_path+'glovefiles/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "\n",
        "\n",
        "for word, i in t.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LRi4o3ZspDFU"
      },
      "source": [
        "# Try the sequential model approach and report the accuracy score. [10 marks]  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zSZDnPWkw2ZZ"
      },
      "source": [
        "### Import layers from Keras to build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5AgwQsfMrzAQ",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, TimeDistributed, Activation,Bidirectional,Dropout, concatenate, LSTM\n",
        "from tensorflow.keras.layers import Flatten, Permute, Input, Add\n",
        "from tensorflow.keras.optimizers import SGD\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gpkVhIbx3gr1"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x27JLS6jAWq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "4d6a2d45-72f9-43a0-a2bc-ea8c764623ff"
      },
      "source": [
        "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
        "embedded_sequences = Embedding(vocab_size, 100, input_length=(MAX_SENT_LENGTH,), weights=[embedding_matrix])(sentence_input)\n",
        "\n",
        "bi_lstm = Bidirectional(LSTM(64, dropout=0.3, activation='tanh', recurrent_dropout=0.3, return_sequences=True))(embedded_sequences)\n",
        "l_dense = Flatten()(TimeDistributed(Dense(100))(bi_lstm))\n",
        "sentenceEncoder = Model(sentence_input, l_dense)\n",
        "\n",
        "article_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH,), dtype='int32')\n",
        "article_encoder = TimeDistributed(sentenceEncoder)(article_input)\n",
        "bi_lstm_article = Bidirectional(LSTM(64, dropout=0.3, activation='tanh', recurrent_dropout=0.3, return_sequences=True))(article_encoder)\n",
        "article_dense_sent = Flatten()((TimeDistributed(Dense(100))(bi_lstm_article)))\n",
        "\n",
        "heading_input = Input(shape=(MAX_SENTS_HEADING,MAX_SENT_LENGTH,), dtype='int32')\n",
        "heading_encoder = TimeDistributed(sentenceEncoder)(heading_input)\n",
        "bi_lstm_heading = LSTM(64, dropout=0.3, activation='tanh', recurrent_dropout=0.3, return_sequences=True)(heading_encoder)\n",
        "heading_dense_sent = Flatten()((TimeDistributed(Dense(100))(bi_lstm_heading)))\n",
        "\n",
        "article_output = concatenate([article_dense_sent, heading_dense_sent], name='concatenate_heading')\n",
        "\n",
        "news_vector = Dense(100, activation='relu')(article_output)\n",
        "preds = Dense(4, activation='softmax')(news_vector)\n",
        "merged_model = Model([article_input, heading_input], [preds])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRI3RcaIDSdo",
        "colab_type": "code",
        "outputId": "b406b203-2e97-4b40-8615-714b1a416f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Concatenate the layers\n",
        "\n",
        "mergedOut = Dense(4, activation='softmax')(article_output)\n",
        "merged_model = Model([article_input,heading_input], mergedOut)\n",
        "merged_model.summary()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 20, 20)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1, 20)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 20, 2000)     2786780     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 1, 2000)      2786780     input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 20, 128)      1057280     time_distributed_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 1, 64)        528640      time_distributed_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 20, 100)      12900       bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_4 (TimeDistrib (None, 1, 100)       6500        lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 2000)         0           time_distributed_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 100)          0           time_distributed_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_heading (Concatenat (None, 2100)         0           flatten_1[0][0]                  \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 4)            8404        concatenate_heading[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 4,400,504\n",
            "Trainable params: 4,400,504\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C5Xrd-JQ3id7"
      },
      "source": [
        "### Compile and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MlduHU2CovxC",
        "outputId": "354873f8-5572-4416-a991-6f116c020d2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "filepath = \"saved-model-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=False, mode='max')\n",
        "\n",
        "merged_model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "merged_model.fit([X_train,X_heading_train], y_train, epochs=5, validation_data=([X_val,X_heading_val], y_val),batch_size = 128,callbacks=[checkpoint])  "
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 32000 samples, validate on 8000 samples\n",
            "Epoch 1/5\n",
            "31872/32000 [============================>.] - ETA: 2s - loss: 0.9099 - acc: 0.6239\n",
            "Epoch 00001: saving model to saved-model-01-0.77.hdf5\n",
            "32000/32000 [==============================] - 672s 21ms/sample - loss: 0.9087 - acc: 0.6245 - val_loss: 0.5964 - val_acc: 0.7705\n",
            "Epoch 2/5\n",
            "31872/32000 [============================>.] - ETA: 2s - loss: 0.5862 - acc: 0.7741\n",
            "Epoch 00002: saving model to saved-model-02-0.82.hdf5\n",
            "32000/32000 [==============================] - 654s 20ms/sample - loss: 0.5864 - acc: 0.7740 - val_loss: 0.4891 - val_acc: 0.8198\n",
            "Epoch 3/5\n",
            "31872/32000 [============================>.] - ETA: 2s - loss: 0.4822 - acc: 0.8192\n",
            "Epoch 00003: saving model to saved-model-03-0.84.hdf5\n",
            "32000/32000 [==============================] - 656s 21ms/sample - loss: 0.4813 - acc: 0.8196 - val_loss: 0.4338 - val_acc: 0.8380\n",
            "Epoch 4/5\n",
            "31872/32000 [============================>.] - ETA: 2s - loss: 0.4354 - acc: 0.8377\n",
            "Epoch 00004: saving model to saved-model-04-0.84.hdf5\n",
            "32000/32000 [==============================] - 661s 21ms/sample - loss: 0.4353 - acc: 0.8375 - val_loss: 0.4185 - val_acc: 0.8421\n",
            "Epoch 5/5\n",
            "31872/32000 [============================>.] - ETA: 2s - loss: 0.3998 - acc: 0.8529\n",
            "Epoch 00005: saving model to saved-model-05-0.86.hdf5\n",
            "32000/32000 [==============================] - 665s 21ms/sample - loss: 0.3994 - acc: 0.8530 - val_loss: 0.4000 - val_acc: 0.8555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8e5045d9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CM3yCmjQoCM3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c874e43-7f24-47e3-e5dd-111b3b462663"
      },
      "source": [
        "score = merged_model.evaluate([X_val,X_heading_val], y_val, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 85.55%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6_OIQkNH3UY",
        "colab_type": "text"
      },
      "source": [
        "##Conclusion:\n",
        "We can observe that the accuracy has improved and the the loss has reduced with balanced data. Running it for a few more epoch might have shown more improvement. Also changing some of the parameters and introducing drop out layers might improve the model. "
      ]
    }
  ]
}