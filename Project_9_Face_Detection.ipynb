{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 9_Face Detection.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lauvshree/ASLADigitechLearningMaterial/blob/master/Project_9_Face_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84w-HTRgCIQa",
        "colab_type": "text"
      },
      "source": [
        "In this problem we use \"Transfer Learning\" of an Object Detector model to detect any object according to the problem in hand.\n",
        "Here, We are particularly interested in detecting faces in a given image.\n",
        "\n",
        "To use the model first, we need to import the model and its supporting files for the model to function. We have the MobileNet model given in file mn_model.py\n",
        "We use the below steps to import the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF7tRfdPB6dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTzG-X-mCh4x",
        "colab_type": "code",
        "outputId": "700cc11f-69eb-4839-c3c5-c42bfd3b7bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx4-nBewCkiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Project 9/Files_required_for_face_detection')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNlSomGZC67h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17IqMVuxDmPb",
        "colab_type": "code",
        "outputId": "ed904b91-b885-4564-9c3e-d7f68a18fb87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/Project\\ 9/Files_required_for_face_detection"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Project 9/Files_required_for_face_detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0pAB1yMEbCN",
        "colab_type": "text"
      },
      "source": [
        "The mn_model uses the older version of keras. We will ignore the warning for the time being and continue to use the older libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gum0tzhoEw1Z",
        "colab_type": "text"
      },
      "source": [
        "Import the BatchGenerator and SSDLoss functions in given files face_generator.py, keras_ssd_loss and ssd_box_encode_decode_utils.py as well, used in MobileNet model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZy4pM-3FEW_",
        "colab_type": "code",
        "outputId": "0357c5b3-bdf6-44df-aeed-7834dca13fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        }
      },
      "source": [
        "#### Import the BatchGenerator and SSDLoss functions as well, used in MobileNet model\n",
        "\n",
        "from face_generator import BatchGenerator\n",
        "from keras_ssd_loss import SSDLoss\n",
        "from ssd_box_encode_decode_utils import SSDBoxEncoder, decode_y, decode_y2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-945bb250791c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mface_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras_ssd_loss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSSDLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mssd_box_encode_decode_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSSDBoxEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_y2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/Project 9/Files_required_for_face_detection/face_generator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mresize_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'imresize'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKc7nJM0FIMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import Adam, SGD, Nadam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, LearningRateScheduler\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K \n",
        "from keras.models import load_model\n",
        "from math import ceil \n",
        "import numpy as np \n",
        "from termcolor import colored\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0enolUQFp56",
        "colab_type": "text"
      },
      "source": [
        "Set the parameters for the model. Originally it will have many classes. Our objective here is only to detect the object. We need to customize the model parameters according to our problem as given below.\n",
        "Set n_classes (no.of classes) = 2, as we are interested in only face detection. Face will be one class and everything else comes under other class (we can call it as background)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAdesq0JFeJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_height =512\n",
        "img_width = 512\n",
        "\n",
        "img_channels = 3\n",
        "\n",
        "n_classes =2 \n",
        "class_names = [\"background\",\"face\"]\n",
        "\n",
        "scales = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # anchorboxes for coco dataset\n",
        "aspect_ratios = [[0.5, 1.0, 2.0],\n",
        "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
        "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
        "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
        "                 [0.5, 1.0, 2.0],\n",
        "                 [0.5, 1.0, 2.0]] # The anchor box aspect ratios used in the original SSD300\n",
        "two_boxes_for_ar1 = True\n",
        "limit_boxes = True # Whether or not you want to limit the anchor boxes to lie entirely within the image boundaries\n",
        "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are scaled as in the original implementation\n",
        "coords = 'centroids' # Whether the box coordinates to be used as targets for the model should be in the 'centroids' or 'minmax' format, see documentation\n",
        "normalize_coords = True\n",
        "\n",
        "det_model_path = \"./\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay4zXqi0GNA1",
        "colab_type": "text"
      },
      "source": [
        "Now, we have imported the model and its dependencies. The next thing is to import the dataset for the model to train on. For this, we are using the WIDER FACE dataset.\n",
        "To make the dataset available follow the steps given below.\n",
        "Create a folder in your google drive for this project.\n",
        "\n",
        "Download the train and test dataset files given in .zip format into your drive folder you created for the project in step-1.\n",
        "\n",
        "Set the project path variable according to the folders you created to use for this project in your google drive.\n",
        "\n",
        "project_path = \"/content/drive/My Drive/DLCP/\"\n",
        "\n",
        "Now, as we mount the drive the images will be available to use for training and testing but in zip format.\n",
        "\n",
        "So, lets extract the images from the zipfiles by using the code given of zipfile module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5Hae9r3Fs0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "project_path = \"/content/gdrive/My Drive/Project 9/Files_required_for_face_detection/dataset/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpTSkWrZGrHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images_path = project_path + 'WIDER_train.zip'\n",
        "test_images_path = project_path + 'WIDER_val.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4zSszz3IQST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "archive = zipfile.ZipFile(train_images_path, 'r')\n",
        "archive.extractall()\n",
        "\n",
        "\n",
        "classnames = []\n",
        "for file in archive.filelist:\n",
        "  fn = file.filename\n",
        "  if (fn.startswith(\"WIDER_train/images/\") and fn.endswith(\".jpg\") == False and fn.find(\"--\") != -1):\n",
        "    hyphenIdx = fn.find(\"--\") + 2\n",
        "    classnames.append(fn[hyphenIdx:len(fn)-1])\n",
        "print(\"Possible objects\")\n",
        "print(classnames)\n",
        "\n",
        "\n",
        "import zipfile\n",
        "archive = zipfile.ZipFile(test_images_path, 'r')\n",
        "archive.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1TCxH5MJMpG",
        "colab_type": "text"
      },
      "source": [
        "Now, the images and their respective labels are available. But the objective of this project is not recognition but it is detection. So, as mentioned above, we will have just two classes viz., background and face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYUk8yAjJe4y",
        "colab_type": "text"
      },
      "source": [
        "Let's load the '' wider_train_small.npy'' file given to check the information given about the dataset. In this file you can see the information about each image in the dataset in a list with following elemets:\n",
        "\n",
        "    1.   Image filename (str)\n",
        "    2.   Image filename (str)\n",
        "    3.   Image size (list) [height, width]\n",
        "    4.   List of bounding box co-ordinates and Class label (list) [[a,b,c,d], Class label, ...]\n",
        " \n",
        "    where,\n",
        "    a,b,c,d are the four co-ordinates of the bounding box\n",
        "    Class label is the position of object as mentioned in `class_names` list above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI9rzbV_Jf6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename1 = '/content/gdrive/My Drive/Project 9/Files_required_for_face_detection/wider_train_small.npy'\n",
        "\n",
        "print(filename1)\n",
        "\n",
        "data = np.load(filename1,allow_pickle=True).item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZlXnpEFJrvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Printed first element to check the above given information.\n",
        "\n",
        "for key in data:\n",
        "  print data[key]\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIsouZSVL83f",
        "colab_type": "text"
      },
      "source": [
        "In the case of the sample data printed out the class name is '1', indicating it is a face. As we can see from the above output all the information mentioned above is there for all the images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cGFrv_GMgoc",
        "colab_type": "text"
      },
      "source": [
        "Now, load the files wider_trian.npy and wider_val.npy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsYazIesKHZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = '/content/gdrive/My Drive/Project 9/Files_required_for_face_detection/wider_train_small.npy'\n",
        "test_data = '/content/gdrive/My Drive/Project 9/Files_required_for_face_detection/wider_val_small.npy'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU57qOaaM4i2",
        "colab_type": "text"
      },
      "source": [
        "Now, call the imported model with the given parameters and freeze all the layers in the model with names not having ''detection'' word as prefix.\n",
        "As we are not training the model from scratch, we are freezing all the above layers in the model having only last few layers while training to update their weights according to the problem in hand. This is called as Transfer Learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aDKsvHRM5mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mn_model import mn_model\n",
        "K.clear_session()\n",
        "model, model_layer, img_input, predictor_sizes = mn_model(image_size=(img_height, img_width, img_channels), \n",
        "                                                                      n_classes = n_classes,\n",
        "                                                                      min_scale = None, \n",
        "                                                                      max_scale = None, \n",
        "                                                                      scales = scales, \n",
        "                                                                      aspect_ratios_global = None, \n",
        "                                                                      aspect_ratios_per_layer = aspect_ratios, \n",
        "                                                                      two_boxes_for_ar1= two_boxes_for_ar1, \n",
        "                                                                      limit_boxes=limit_boxes, \n",
        "                                                                      variances= variances, \n",
        "                                                                      coords=coords, \n",
        "                                                                      normalize_coords=normalize_coords)\n",
        "\n",
        "print (\"Freezing classification layers\")\n",
        "#Freeze layers\n",
        "for layer_key in model_layer:\n",
        "  if('detection'  not in layer_key):\n",
        "    model_layer[layer_key].trainable = False\n",
        "    print('Freezing layer, %s' % layer_key)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaoS7AY1OXMV",
        "colab_type": "text"
      },
      "source": [
        "After making the model ready for transfer learning, load the weights of the model given in file ''mobilenet_1_0_224_tf.h5''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTLVRu0ROYBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"Loading classification weights\")\n",
        "classification_model = '/content/gdrive/My Drive/Project 9/Files_required_for_face_detection/mobilenet_1_0_224_tf.h5'\n",
        "model.load_weights(classification_model,  by_name= True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpCdsZfqPp-A",
        "colab_type": "text"
      },
      "source": [
        "Using the functions given in the model, we are trying to divide the dataset into train and validation samples. Run the below code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOr7YqNnPjUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "ssd_box_encoder = SSDBoxEncoder(img_height=img_height,\n",
        "                                img_width=img_width,\n",
        "                                n_classes=n_classes, \n",
        "                                predictor_sizes=predictor_sizes,\n",
        "                                min_scale=None,\n",
        "                                max_scale=None,\n",
        "                                scales=scales,\n",
        "                                aspect_ratios_global=None,\n",
        "                                aspect_ratios_per_layer=aspect_ratios,\n",
        "                                two_boxes_for_ar1=two_boxes_for_ar1,\n",
        "                                limit_boxes=limit_boxes,\n",
        "                                variances=variances,\n",
        "                                pos_iou_threshold=0.5,\n",
        "                                neg_iou_threshold=0.2,\n",
        "                                coords=coords,\n",
        "                                normalize_coords=normalize_coords)\n",
        "\n",
        "train_dataset = BatchGenerator(images_path=train_data, \n",
        "                include_classes='all', \n",
        "                box_output_format = ['class_id', 'xmin', 'xmax', 'ymin', 'ymax'])\n",
        "\n",
        "print (\"==>TRAINING DATA\")\n",
        "print (\"==> Parsing XML files ...\")\n",
        "\n",
        "train_dataset.parse_xml(\n",
        "                  annotations_path=train_data,\n",
        "                  image_set_path='None',\n",
        "                  image_set='None',\n",
        "                  classes = class_names, \n",
        "                  exclude_truncated=False,\n",
        "                  exclude_difficult=False,\n",
        "                  ret=False, \n",
        "                  debug = False)\n",
        "print(\"==>Parsing XML Finished.\")\n",
        "\n",
        "print (\"==>Generate training batches...\")\n",
        "train_generator = train_dataset.generate(\n",
        "                 batch_size=batch_size,\n",
        "                 train=True,\n",
        "                 ssd_box_encoder=ssd_box_encoder,\n",
        "                 equalize=True,\n",
        "                 brightness=(0.5,2,0.5),\n",
        "                 flip=0.5,\n",
        "                 translate=((0, 20), (0, 30), 0.5),\n",
        "                 scale=(0.75, 1.2, 0.5),\n",
        "                 crop=False,\n",
        "                 #random_crop = (img_height,img_width,1,3), \n",
        "                 random_crop=False,\n",
        "                 resize=(img_height, img_width),\n",
        "                 #resize=False,\n",
        "                 gray=False,\n",
        "                 limit_boxes=True,\n",
        "                 include_thresh=0.4,\n",
        "                 diagnostics=False)\n",
        "\n",
        "print (\"==>Training batch generation complete\")\n",
        "\n",
        "print(train_dataset.filenames)\n",
        "\n",
        "n_train_samples = train_dataset.get_n_samples()\n",
        "\n",
        "print (\"==>Total number of training samples = {}\".format(n_train_samples))\n",
        "\n",
        "print (\"==>VALIDATION\")\n",
        "\n",
        "val_dataset = BatchGenerator(images_path=test_data, include_classes='all', \n",
        "                box_output_format = ['class_id', 'xmin', 'xmax', 'ymin', 'ymax'])\n",
        "\n",
        "print (\"==> Parsing XML files ...\")\n",
        "\n",
        "val_dataset.parse_xml(\n",
        "                  annotations_path=test_data,\n",
        "                  image_set_path='None',\n",
        "                  image_set='None',\n",
        "                  classes = class_names, \n",
        "                  exclude_truncated=False,\n",
        "                  exclude_difficult=False,\n",
        "                  ret=False, \n",
        "                  debug = False)\n",
        "\n",
        "\n",
        "print(\"==>Parsing XML Finished.\")\n",
        "\n",
        "\n",
        "print (\"==>Generate testing batches...\")\n",
        "val_generator = val_dataset.generate(\n",
        "                 batch_size=batch_size,\n",
        "                 train=True,\n",
        "                 ssd_box_encoder=ssd_box_encoder,\n",
        "                 equalize=False,\n",
        "                 brightness=False,\n",
        "                 flip=False,\n",
        "                 translate=False,\n",
        "                 scale=False,\n",
        "                 crop=False,\n",
        "                 #random_crop = (img_height,img_width,1,3), \n",
        "                 random_crop=False, \n",
        "                 resize=(img_height, img_width), \n",
        "                 #resize=False, \n",
        "                 gray=False,\n",
        "                 limit_boxes=True,\n",
        "                 include_thresh=0.4,\n",
        "                 diagnostics=False)\n",
        "\n",
        "\n",
        "print (\"==>Training batch generation complete\")\n",
        "\n",
        "n_val_samples = val_dataset.get_n_samples()\n",
        "\n",
        "print (\"==>Total number of validation samples = {}\".format(n_val_samples))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr-nDR0SQyk-",
        "colab_type": "text"
      },
      "source": [
        "Now, lets setup things for training by initilaizing required variables like learning rate, epochs, optimizer and loss function(SSDLoss) to compile the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIPjl3TUQzFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setting up training \n",
        "\n",
        "batch_size = 16\n",
        "num_epochs = 25\n",
        "\n",
        "#Learning rate\n",
        "base_lr = 0.002\n",
        "\n",
        "# Optimizer\n",
        "adam = Adam(lr=base_lr, beta_1=0.9, beta_2=0.999, epsilon=1e-6, decay = 0.0)\n",
        "\n",
        "# Loss\n",
        "ssd_loss = SSDLoss(neg_pos_ratio=2, n_neg_min=0, alpha=1.0, beta = 1.0)\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U8KU6hERBel",
        "colab_type": "text"
      },
      "source": [
        "Lets create early stopping and model checkpoint layers on validation loss with some patience values and use fit_generator to train the model on data generated batch-by-batch by a Python generator, train_generator object as generator, using early stopping and model checkpoint as callbacks.\n",
        "We are using checkpoint to save the best model based on validation accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3h6-vuRRCic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=100)\n",
        "\n",
        "model_checkpoint =  ModelCheckpoint(det_model_path + 'ssd_mobilenet_face_epoch_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
        "                                                           monitor='val_loss',\n",
        "                                                           verbose=1,\n",
        "                                                           save_best_only=True,\n",
        "                                                           save_weights_only=True,\n",
        "                                                           mode='auto',\n",
        "                                                           period=1)\n",
        "\n",
        "print (\"Fitting the model\")\n",
        "\n",
        "history = model.fit_generator(generator = train_generator,\n",
        "                              steps_per_epoch = ceil(n_train_samples/batch_size)*2,\n",
        "                              epochs = num_epochs,\n",
        "                              callbacks = [model_checkpoint, early_stopping],                      \n",
        "                              validation_data = val_generator,\n",
        "                              validation_steps = ceil(n_val_samples/batch_size))\n",
        "\n",
        "model.save_weights(\"./\" + 'ssd_mobilenet_weights_epoch_{}.h5'.format(num_epochs))\n",
        "\n",
        "print (\"model and weight files saved at : \" + det_model_path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow1r-RyPFZi3",
        "colab_type": "text"
      },
      "source": [
        "We will now load the best saved model from above step and check predictions for test data using test_generator object to generate batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8fPLlmkFaYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = './'\n",
        "model_name = './ssd_mobilenet_face_epoch_20_loss0.2117.h5'\n",
        "\n",
        "model.load_weights(model_path + model_name,  by_name= True)\n",
        "\n",
        "print (colored('weights %s loaded' % (model_path + model_name), 'green'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_2T0bgFYr8-",
        "colab_type": "text"
      },
      "source": [
        "Let's use the below function to plot the boundingbox in the test image to show the predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04ORbUlhY7X8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_bb(path, filename, results, prediction=True):\n",
        "  \n",
        "  img = image.load_img(filename, target_size=(img_height, img_width))\n",
        "  img = image.img_to_array(img)\n",
        "\n",
        "  filename = filename.split(\"/\")[-1]\n",
        "\n",
        "  if(not prediction):\n",
        "    filename = filename[:-4] + \"_gt\" + \".jpg\"\n",
        "\n",
        "  currentAxis = plt.gca()\n",
        "\n",
        " # Get detections with confidence higher than 0.6.\n",
        "  colors = plt.cm.hsv(np.linspace(0, 1, 25)).tolist()\n",
        "  color_code = min(len(results), 16)\n",
        "  print (colored(\"total number of bbs: %d\" % len(results), \"yellow\"))\n",
        "  for result in results:\n",
        "    # Parse the outputs.\n",
        "\n",
        "    if(prediction):\n",
        "      det_label = result[0]\n",
        "      det_conf = result[1]\n",
        "      det_xmin = result[2]\n",
        "      det_xmax = result[3]\n",
        "      det_ymin = result[4]\n",
        "      det_ymax = result[5]\n",
        "    else :\n",
        "      det_label = result[0]\n",
        "      det_xmin = result[1]\n",
        "      det_xmax = result[2]\n",
        "      det_ymin = result[3]\n",
        "      det_ymax = result[4]\n",
        "\n",
        "    xmin = int(det_xmin)\n",
        "    ymin = int(det_ymin)\n",
        "    xmax = int(det_xmax)\n",
        "    ymax = int(det_ymax)\n",
        "\n",
        "    if(prediction):\n",
        "      score = det_conf\n",
        "    \n",
        "    plt.imshow(img / 255.)\n",
        "    \n",
        "    label = int(int(det_label))\n",
        "    label_name = class_names[label]\n",
        "    # print label_name \n",
        "    # print label\n",
        "\n",
        "    if(prediction):\n",
        "      display_txt = '{:0.2f}'.format(score)\n",
        "    else:\n",
        "      display_txt = '{}'.format(label_name)\n",
        "\n",
        "      \n",
        "    # print (xmin, ymin, ymin, ymax)\n",
        "    coords = (xmin, ymin), (xmax-xmin), (ymax-ymin)\n",
        "    color_code = color_code-1 \n",
        "    color = colors[color_code]\n",
        "    currentAxis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
        "    currentAxis.text(xmin, ymin, display_txt, bbox={'facecolor':color, 'alpha':0.2})\n",
        "\n",
        "  # y\n",
        "  currentAxis.axes.get_yaxis().set_visible(False)\n",
        "  # x\n",
        "  currentAxis.axes.get_xaxis().set_visible(False)\n",
        "  plt.savefig(path + filename, bbox_inches='tight')\n",
        "\n",
        "  print ('saved' , path + filename)\n",
        "\n",
        "  plt.clf()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgiy3skTZ2M8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir output_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YevhbqLGZ8QF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "test_size = 10\n",
        "test_generator = val_dataset.generate(\n",
        "                 batch_size=test_size,\n",
        "                 train=False,\n",
        "                 ssd_box_encoder=ssd_box_encoder,\n",
        "                 equalize=False,\n",
        "                 brightness=False,\n",
        "                 flip=False,\n",
        "                 translate=False,\n",
        "                 scale=False,\n",
        "                 crop=False,\n",
        "                 #random_crop = (img_height,img_width,1,3), \n",
        "                 random_crop=False, \n",
        "                 resize=(img_height, img_width), \n",
        "                 #resize=False,\n",
        "                 gray=False,\n",
        "                 limit_boxes=True,\n",
        "                 include_thresh=0.4,\n",
        "                 diagnostics=False)\n",
        "\n",
        "print (colored(\"done.\", \"green\"))\n",
        "\n",
        "print (colored(\"now predicting...\", \"yellow\"))\n",
        "\n",
        "_CONF = 0.60 \n",
        "_IOU = 0.15\n",
        "\n",
        "for i in range(test_size):\n",
        "  X, y, filenames = next(test_generator)\n",
        "  y_pred = model.predict(X)\n",
        "\n",
        "\n",
        "  y_pred_decoded = decode_y2(y_pred,\n",
        "                             confidence_thresh=_CONF,\n",
        "                            iou_threshold=_IOU,\n",
        "                            top_k='all',\n",
        "                            input_coords=coords,\n",
        "                            normalize_coords=normalize_coords,\n",
        "                            img_height=img_height,\n",
        "                            img_width=img_width)\n",
        "\n",
        "\n",
        "  np.set_printoptions(suppress=True)\n",
        "\n",
        "  save_bb(\"./output_test/\", filenames[i], y_pred_decoded[i])\n",
        "  save_bb(\"./output_test/\", filenames[i], y[i], prediction=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWrnD_0jcnpd",
        "colab_type": "text"
      },
      "source": [
        "Let's now visualize the test image to check the predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zte2YUk3cxCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0pTUEUsczEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "img = cv2.imread('././output_test/28_Sports_Fan_Sports_Fan_28_590_gt.jpg', cv2.IMREAD_UNCHANGED)\n",
        "cv2_imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLMkbl1NinaP",
        "colab_type": "text"
      },
      "source": [
        "As we can see in the above image all the faces of which even a part are shown have been predicted by our model correctly. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2HMBQa9hf5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pylab inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "img=mpimg.imread('./output_test/2_Demonstration_Political_Rally_2_219.jpg')\n",
        "figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
        "imgplot = plt.imshow(img)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ-86CaLkFAG",
        "colab_type": "text"
      },
      "source": [
        "This image prediction seems to have gone wrong. The training set didn't have a balance of ethnicity and that could be the main reason. We need to balance the data on demographic basis. We will verify the test results with another image in the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH2dHEUoiQ02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img=mpimg.imread('/content/gdrive/My Drive/Project 9/Files_required_for_face_detection/WIDER_val/images/16--Award_Ceremony/16_Award_Ceremony_Awards_Ceremony_16_338.jpg')\n",
        "figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
        "imgplot = plt.imshow(img)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "img=mpimg.imread('./output_test/16_Award_Ceremony_Awards_Ceremony_16_338.jpg')\n",
        "figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
        "imgplot = plt.imshow(img)\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqD10C4aA_Ad",
        "colab_type": "text"
      },
      "source": [
        "The face detection in this case given the original image seems correct. We can however fine tune the model by training it with a more extensice dataset. "
      ]
    }
  ]
}